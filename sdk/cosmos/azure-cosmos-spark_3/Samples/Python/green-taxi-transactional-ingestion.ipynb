{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b41802-d650-497a-b054-005bf7aa94ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# No external packages needed - using synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2339ca74-800a-41ad-b3d4-084c5fe270b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# NYC Green Taxi Data Ingestion - Transactional Batch Mode\n",
    "\n",
    "This notebook demonstrates ingesting NYC Green Taxi dataset into Azure Cosmos DB using **transactional batch mode**.\n",
    "\n",
    "**Configuration:**\n",
    "- Authentication: Managed Identity (Azure Databricks)\n",
    "- Same data preparation as normal bulk mode for fair comparison\n",
    "- Only difference: `spark.cosmos.write.bulk.transactional = true`\n",
    "\n",
    "**Key Requirements for Transactional Batches:**\n",
    "- All operations within a batch must be for the same partition key\n",
    "- Maximum 100 operations per batch\n",
    "- Maximum 2MB per batch\n",
    "- All operations succeed or all fail atomically\n",
    "\n",
    "Dataset: [Azure Open Datasets - NYC Green Taxi](https://learn.microsoft.com/en-gb/azure/open-datasets/dataset-taxi-green)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62503f93-ac39-408e-a5c0-05a8cabc785b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration\n",
    "Update these values with your Cosmos DB account details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aca37d5-d81d-4982-b59a-34e384c5e54e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cosmos DB Configuration - Managed Identity\n",
    "config = {\n",
    "    \"spark.cosmos.accountEndpoint\": \"https://tvk-my-cosmos-account.documents.azure.com:443/\",\n",
    "    \"spark.cosmos.database\": \"spark-load-tests\",\n",
    "    \"spark.cosmos.container\": \"transactional-bulk\",\n",
    "    \n",
    "    # Use MI (no client secret)\n",
    "    \"spark.cosmos.auth.type\": \"ManagedIdentity\",\n",
    "    \n",
    "    # Required for ARM metadata lookup\n",
    "    \"spark.cosmos.account.subscriptionId\": \"220fc532-6091-423c-8ba0-66c2397d591b\",\n",
    "    \"spark.cosmos.account.resourceGroupName\": \"tvk-mgt-tests\",\n",
    "    \"spark.cosmos.account.tenantId\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "338b6ff3-4e87-44c7-b222-410c611befb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record limit: 1000000\nMax records per partition: 10\nWill generate 100000 partitions (hours) with ~10 records each\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# PERFORMANCE TEST CONFIGURATION\n",
    "# ========================================\n",
    "# Total records to generate\n",
    "RECORD_LIMIT = 1000000  # Change this value to control data volume\n",
    "\n",
    "# Minimum and maximum records per partition (logical partition key)\n",
    "# IMPORTANT: For transactional batches, must be <= 100\n",
    "# Provide a range to vary partition sizes. Set MIN_RECORDS_PER_PARTITION and MAX_RECORDS_PER_PARTITION.\n",
    "# Examples:\n",
    "#   MIN=5, MAX=10  - Variable small batches\n",
    "#   MIN=50, MAX=90 - Variable medium batches\n",
    "MIN_RECORDS_PER_PARTITION = 10\n",
    "MAX_RECORDS_PER_PARTITION = 10  # Ensures no partition exceeds this count\n",
    "\n",
    "import random\n",
    "\n",
    "# Build partition size list using random sizes within the range while summing to RECORD_LIMIT\n",
    "partition_sizes = []\n",
    "remaining = RECORD_LIMIT\n",
    "while remaining > 0:\n",
    "    sz = random.randint(MIN_RECORDS_PER_PARTITION, MAX_RECORDS_PER_PARTITION)\n",
    "    if sz > remaining:\n",
    "        sz = remaining\n",
    "    partition_sizes.append(sz)\n",
    "    remaining -= sz\n",
    "\n",
    "num_partitions = len(partition_sizes)\n",
    "\n",
    "print(f\"Record limit: {RECORD_LIMIT}\")\n",
    "print(f\"Partition size range: {MIN_RECORDS_PER_PARTITION}-{MAX_RECORDS_PER_PARTITION}\")\n",
    "print(f\"Will generate {num_partitions} partitions (hours) with variable records per partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8359fef-65fb-405f-a046-f839e6709708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Synthetic Taxi Trip Data\n",
    "\n",
    "Creating realistic synthetic data - SAME AS NORMAL MODE for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b07c1a6-92af-4da3-a277-61ed64e42bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000000 synthetic taxi trip records...\nDistributing across 100000 partitions with max 10 records each\nGenerated 1000000 records\n\n✓ Partition distribution (records per hour):\n+-------------+-----+\n|hour         |count|\n+-------------+-----+\n|2022-01-01-00|10   |\n|2022-01-01-01|10   |\n|2022-01-01-02|10   |\n|2022-01-01-03|10   |\n|2022-01-01-04|10   |\n|2022-01-01-05|10   |\n|2022-01-01-06|10   |\n|2022-01-01-07|10   |\n|2022-01-01-08|10   |\n|2022-01-01-09|10   |\n|2022-01-01-10|10   |\n|2022-01-01-11|10   |\n|2022-01-01-12|10   |\n|2022-01-01-13|10   |\n|2022-01-01-14|10   |\n|2022-01-01-15|10   |\n|2022-01-01-16|10   |\n|2022-01-01-17|10   |\n|2022-01-01-18|10   |\n|2022-01-01-19|10   |\n|2022-01-01-20|10   |\n|2022-01-01-21|10   |\n|2022-01-01-22|10   |\n|2022-01-01-23|10   |\n|2022-01-02-00|10   |\n|2022-01-02-01|10   |\n|2022-01-02-02|10   |\n|2022-01-02-03|10   |\n|2022-01-02-04|10   |\n|2022-01-02-05|10   |\n|2022-01-02-06|10   |\n|2022-01-02-07|10   |\n|2022-01-02-08|10   |\n|2022-01-02-09|10   |\n|2022-01-02-10|10   |\n|2022-01-02-11|10   |\n|2022-01-02-12|10   |\n|2022-01-02-13|10   |\n|2022-01-02-14|10   |\n|2022-01-02-15|10   |\n|2022-01-02-16|10   |\n|2022-01-02-17|10   |\n|2022-01-02-18|10   |\n|2022-01-02-19|10   |\n|2022-01-02-20|10   |\n|2022-01-02-21|10   |\n|2022-01-02-22|10   |\n|2022-01-02-23|10   |\n|2022-01-03-00|10   |\n|2022-01-03-01|10   |\n|2022-01-03-02|10   |\n|2022-01-03-03|10   |\n|2022-01-03-04|10   |\n|2022-01-03-05|10   |\n|2022-01-03-06|10   |\n|2022-01-03-07|10   |\n|2022-01-03-08|10   |\n|2022-01-03-09|10   |\n|2022-01-03-10|10   |\n|2022-01-03-11|10   |\n|2022-01-03-12|10   |\n|2022-01-03-13|10   |\n|2022-01-03-14|10   |\n|2022-01-03-15|10   |\n|2022-01-03-16|10   |\n|2022-01-03-17|10   |\n|2022-01-03-18|10   |\n|2022-01-03-19|10   |\n|2022-01-03-20|10   |\n|2022-01-03-21|10   |\n|2022-01-03-22|10   |\n|2022-01-03-23|10   |\n|2022-01-04-00|10   |\n|2022-01-04-01|10   |\n|2022-01-04-02|10   |\n|2022-01-04-03|10   |\n|2022-01-04-04|10   |\n|2022-01-04-05|10   |\n|2022-01-04-06|10   |\n|2022-01-04-07|10   |\n|2022-01-04-08|10   |\n|2022-01-04-09|10   |\n|2022-01-04-10|10   |\n|2022-01-04-11|10   |\n|2022-01-04-12|10   |\n|2022-01-04-13|10   |\n|2022-01-04-14|10   |\n|2022-01-04-15|10   |\n|2022-01-04-16|10   |\n|2022-01-04-17|10   |\n|2022-01-04-18|10   |\n|2022-01-04-19|10   |\n|2022-01-04-20|10   |\n|2022-01-04-21|10   |\n|2022-01-04-22|10   |\n|2022-01-04-23|10   |\n|2022-01-05-00|10   |\n|2022-01-05-01|10   |\n|2022-01-05-02|10   |\n|2022-01-05-03|10   |\n+-------------+-----+\nonly showing top 100 rows\n\n\n✓ Maximum records in any partition: 10 (limit: 10)\n✓ All partitions are within limits!\n+--------+-------------------+-------------------+--------------+------------+----------+-----+---------+-----------+-----------------+\n|vendorID| lpepPickupDatetime|lpepDropoffDatetime|passengerCount|tripDistance|fareAmount|extra|tipAmount|tollsAmount|      totalAmount|\n+--------+-------------------+-------------------+--------------+------------+----------+-----+---------+-----------+-----------------+\n|       1|2022-01-01 00:40:00|2022-01-01 00:52:00|             6|        5.86|     20.62| 1.47|    10.15|       4.46|             36.7|\n|       2|2022-01-01 00:05:00|2022-01-01 00:47:00|             1|        1.08|      20.3| 1.01|      0.4|       0.99|             22.7|\n|       2|2022-01-01 00:41:00|2022-01-01 01:30:00|             2|        9.26|     24.47| 1.74|    11.38|        0.8|38.38999999999999|\n|       2|2022-01-01 00:27:00|2022-01-01 00:53:00|             2|         4.7|     58.44|  0.2|      5.7|       1.79|66.13000000000001|\n|       2|2022-01-01 00:22:00|2022-01-01 01:05:00|             1|       14.73|     42.54| 1.95|     5.68|       2.76|            52.93|\n+--------+-------------------+-------------------+--------------+------------+----------+-----+---------+-----------+-----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility - SAME AS NORMAL MODE\n",
    "random.seed(42)\n",
    "\n",
    "# Generate synthetic taxi trip data with controlled partition distribution\n",
    "print(f\"Generating {RECORD_LIMIT} synthetic taxi trip records...\")\n",
    "print(f\"Distributing across {num_partitions} partitions with max {MAX_RECORDS_PER_PARTITION} records each\")\n",
    "\n",
    "# Create realistic taxi trip data with controlled distribution\n",
    "data = []\n",
    "base_time = datetime(2022, 1, 1, 0, 0, 0)\n",
    "\n",
    "# Distribute records according to partition_sizes\n",
    "partition_index = 0\n",
    "for sz in partition_sizes:\n",
    "    for _ in range(sz):\n",
    "        # Generate pickup time for this partition\n",
    "        pickup_time = base_time + timedelta(hours=partition_index, minutes=random.randint(0, 59))\n",
    "        trip_duration = random.randint(5, 60)  # 5-60 minutes\n",
    "        dropoff_time = pickup_time + timedelta(minutes=trip_duration)\n",
    "        data.append(Row(\n",
    "            vendorID=random.choice([1, 2]),\n",
    "            lpepPickupDatetime=pickup_time,\n",
    "            lpepDropoffDatetime=dropoff_time,\n",
    "            passengerCount=random.randint(1, 6),\n",
    "            tripDistance=round(random.uniform(0.5, 20.0), 2),\n",
    "            fareAmount=round(random.uniform(5.0, 75.0), 2),\n",
    "            extra=round(random.uniform(0, 2.0), 2),\n",
    "            tipAmount=round(random.uniform(0, 15.0), 2),\n",
    "            tollsAmount=round(random.uniform(0, 5.0), 2),\n",
    "            totalAmount=0.0  # Will calculate\n",
    "        ))\n",
    "    partition_index += 1\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Calculate total amount\n",
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn(\"totalAmount\", \n",
    "    col(\"fareAmount\") + col(\"extra\") + col(\"tipAmount\") + col(\"tollsAmount\"))\n",
    "\n",
    "print(f\"Generated {df.count()} records\")\n",
    "\n",
    "# Verify partition distribution\n",
    "from pyspark.sql.functions import date_format, count as sql_count\n",
    "partition_counts = df.groupBy(date_format(col(\"lpepPickupDatetime\"), \"yyyy-MM-dd-HH\").alias(\"hour\")) \\\n",
    "    .agg(sql_count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(\"hour\")\n",
    "\n",
    "print(\"\\n✓ Partition distribution (records per hour):\")\n",
    "partition_counts.show(100, truncate=False)\n",
    "\n",
    "max_partition = partition_counts.agg({\"count\": \"max\"}).collect()[0][0]\n",
    "print(f\"\\n✓ Maximum records in any partition: {max_partition} (limit: {MAX_RECORDS_PER_PARTITION})\")\n",
    "\n",
    "if max_partition > 100:\n",
    "    print(\"⚠️ WARNING: Some partitions exceed 100 records - not suitable for transactional batches!\")\n",
    "elif max_partition <= MAX_RECORDS_PER_PARTITION:\n",
    "    print(\"✓ All partitions are within limits!\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c32de058-f12b-4dd3-972f-54cbeaff362b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Data for Cosmos DB\n",
    "\n",
    "Add required fields - SAME AS NORMAL MODE for fair comparison:\n",
    "- `id`: Unique identifier\n",
    "- `partitionKey`: Date-hour format (`yyyy-MM-dd-HH`)\n",
    "- Order by partition key\n",
    "- Repartition by partition key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "356dda77-afbd-4b89-96bb-2e2ba9d4c94c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 1000000 records for transactional batch ingestion\n\nSample data (showing partition key grouping):\n+----------------+-------------+-------------------+----------+------------+\n|              id| partitionKey| lpepPickupDatetime|fareAmount|tripDistance|\n+----------------+-------------+-------------------+----------+------------+\n| 2022-01-01-00-0|2022-01-01-00|2022-01-01 00:40:00|     20.62|        5.86|\n| 2022-01-01-00-1|2022-01-01-00|2022-01-01 00:05:00|      20.3|        1.08|\n| 2022-01-01-00-2|2022-01-01-00|2022-01-01 00:41:00|     24.47|        9.26|\n| 2022-01-01-00-3|2022-01-01-00|2022-01-01 00:27:00|     58.44|         4.7|\n| 2022-01-01-00-4|2022-01-01-00|2022-01-01 00:22:00|     42.54|       14.73|\n| 2022-01-01-00-5|2022-01-01-00|2022-01-01 00:53:00|      9.87|        4.25|\n| 2022-01-01-00-6|2022-01-01-00|2022-01-01 00:54:00|      49.5|        5.92|\n| 2022-01-01-00-7|2022-01-01-00|2022-01-01 00:17:00|     42.39|       12.88|\n| 2022-01-01-00-8|2022-01-01-00|2022-01-01 00:59:00|     58.78|        6.82|\n| 2022-01-01-00-9|2022-01-01-00|2022-01-01 00:25:00|      44.7|       18.31|\n|2022-01-01-01-10|2022-01-01-01|2022-01-01 01:56:00|     22.26|        5.67|\n|2022-01-01-01-11|2022-01-01-01|2022-01-01 01:57:00|     74.83|        4.78|\n|2022-01-01-01-12|2022-01-01-01|2022-01-01 01:07:00|      9.45|        8.73|\n|2022-01-01-01-13|2022-01-01-01|2022-01-01 01:35:00|     52.72|       14.55|\n|2022-01-01-01-14|2022-01-01-01|2022-01-01 01:07:00|     71.77|        9.35|\n+----------------+-------------+-------------------+----------+------------+\nonly showing top 15 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, date_format, concat_ws, monotonically_increasing_id\n",
    "\n",
    "# Add id and partitionKey fields - SAME AS NORMAL MODE\n",
    "df_prepared = df \\\n",
    "    .withColumn(\"partitionKey\", date_format(col(\"lpepPickupDatetime\"), \"yyyy-MM-dd-HH\")) \\\n",
    "    .withColumn(\"id\", concat_ws(\"-\", \n",
    "                                col(\"partitionKey\"),\n",
    "                                monotonically_increasing_id().cast(\"string\")))\n",
    "\n",
    "# Order by partitionKey - same as normal mode\n",
    "df_prepared = df_prepared.orderBy(\"partitionKey\")\n",
    "\n",
    "print(f\"Prepared {df_prepared.count()} records for transactional batch ingestion\")\n",
    "print(\"\\nSample data (showing partition key grouping):\")\n",
    "df_prepared.select(\"id\", \"partitionKey\", \"lpepPickupDatetime\", \"fareAmount\", \"tripDistance\").show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "accfa2d1-aeeb-4944-b162-472a912b474a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write to Cosmos DB - Transactional Batch Mode\n",
    "\n",
    "**Only difference from normal mode:** `spark.cosmos.write.bulk.transactional = true`\n",
    "\n",
    "All other configurations and optimizations are identical for fair performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "076378b9-eab6-4514-9abd-6dfd2746697f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting write to Cosmos DB (transactional batch mode)...\n⚡ Each batch will be atomic - all operations succeed or all fail\n\n✓ Transactional batch write completed in 98.41 seconds\nRecords written: 1000000\nAll batches committed atomically!\n"
     ]
    }
   ],
   "source": [
    "# Cosmos DB write configuration - TRANSACTIONAL BATCH MODE\n",
    "write_config = {\n",
    "    **config,  # Include all Managed Identity config\n",
    "    \"spark.cosmos.write.strategy\": \"ItemOverwrite\",\n",
    "    \"spark.cosmos.write.bulk.enabled\": \"true\",\n",
    "    # ONLY DIFFERENCE: transactional flag set to true\n",
    "    \"spark.cosmos.write.bulk.transactional\": \"true\"\n",
    "}\n",
    "\n",
    "print(\"Starting write to Cosmos DB (transactional batch mode)...\")\n",
    "print(\"⚡ Each batch will be atomic - all operations succeed or all fail\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Apply same repartitioning as normal mode for fair comparison\n",
    "df_prepared.repartition(\"partitionKey\") \\\n",
    "    .write \\\n",
    "    .format(\"cosmos.oltp\") \\\n",
    "    .options(**write_config) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n✓ Transactional batch write completed in {duration:.2f} seconds\")\n",
    "print(f\"Records written: {df_prepared.count()}\")\n",
    "print(\"All batches committed atomically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "562625df-4675-4e51-9c54-173d97f5063f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verify Data in Cosmos DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc1ac34-2ef3-4102-bcc6-6b20705bc81c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in Cosmos DB: 1000000\n\nRecords by partition key:\n+-------------+-----+\n| partitionKey|count|\n+-------------+-----+\n|2022-01-01-00|   10|\n|2022-01-01-01|   10|\n|2022-01-01-02|   10|\n|2022-01-01-03|   10|\n|2022-01-01-04|   10|\n|2022-01-01-05|   10|\n|2022-01-01-06|   10|\n|2022-01-01-07|   10|\n|2022-01-01-08|   10|\n|2022-01-01-09|   10|\n|2022-01-01-10|   10|\n|2022-01-01-11|   10|\n|2022-01-01-12|   10|\n|2022-01-01-13|   10|\n|2022-01-01-14|   10|\n|2022-01-01-15|   10|\n|2022-01-01-16|   10|\n|2022-01-01-17|   10|\n|2022-01-01-18|   10|\n|2022-01-01-19|   10|\n+-------------+-----+\nonly showing top 20 rows\n\n\nSample records:\n+----------------+-------------+------------------+----------+\n|              id| partitionKey|lpepPickupDatetime|fareAmount|\n+----------------+-------------+------------------+----------+\n|2022-01-01-02-20|2022-01-01-02|  1641002820000000|     46.18|\n|2022-01-01-02-21|2022-01-01-02|  1641004800000000|     65.17|\n|2022-01-01-02-22|2022-01-01-02|  1641003180000000|     44.97|\n|2022-01-01-02-23|2022-01-01-02|  1641003120000000|     34.65|\n|2022-01-01-02-24|2022-01-01-02|  1641004860000000|     61.04|\n|2022-01-01-02-25|2022-01-01-02|  1641002880000000|     66.21|\n|2022-01-01-02-26|2022-01-01-02|  1641002760000000|     64.41|\n|2022-01-01-02-27|2022-01-01-02|  1641003900000000|     32.33|\n|2022-01-01-02-28|2022-01-01-02|  1641005040000000|     20.24|\n|2022-01-01-02-29|2022-01-01-02|  1641003600000000|     69.36|\n+----------------+-------------+------------------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Read back from Cosmos DB to verify\n",
    "read_config = {\n",
    "    **config  # Use same Managed Identity config\n",
    "}\n",
    "\n",
    "df_verify = spark.read.format(\"cosmos.oltp\").options(**read_config).load()\n",
    "print(f\"Total records in Cosmos DB: {df_verify.count()}\")\n",
    "\n",
    "# Show distribution by partition\n",
    "print(\"\\nRecords by partition key:\")\n",
    "from pyspark.sql.functions import count\n",
    "df_verify.groupBy(\"partitionKey\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(\"partitionKey\") \\\n",
    "    .show(20)\n",
    "\n",
    "print(\"\\nSample records:\")\n",
    "df_verify.select(\"id\", \"partitionKey\", \"lpepPickupDatetime\", \"fareAmount\").show(10)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "green-taxi-transactional-ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}